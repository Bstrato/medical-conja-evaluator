{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtQz95n9TRdkjOx5qC+9IO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bstrato/medical-conja-evaluator/blob/main/LLM_Reasoning_over_graph_and_tabular.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lx7s_Hi9vOJ-"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "from typing import Dict, List, Tuple, Any, Optional\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import os\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #./medical_conja_finetuned\n",
        "    #Qwen/Qwen2-0.5B-Instruct\n",
        "    #Table-R1/Table-R1-SFT-8B\n",
        "    #PKU-ML/G1-3B\n",
        "    #zzachw12/llemr-v1\n",
        "\n",
        "class EnhancedMedicalConJEvaluator:\n",
        "    \"\"\"Enhanced Medical admission evaluator using Con-J with LLM judge evaluation\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"Qwen/Qwen2-0.5B-Instruct\", seed: int = 2021):\n",
        "        \"\"\"Initialize the evaluator with model and seed configuration\"\"\"\n",
        "        self.seed = seed\n",
        "        self.set_seed()\n",
        "        self.model_name = model_name\n",
        "        self.evaluation_history = []\n",
        "        self.bias_metrics = {}\n",
        "\n",
        "        print(f\"Loading enhanced Medical Con-J evaluator with {model_name}\")\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_name,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "            trust_remote_code=True,\n",
        "            low_cpu_mem_usage=True,\n",
        "            load_in_8bit=False,\n",
        "        )\n",
        "\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        print(\"Enhanced Medical Con-J evaluator loaded successfully!\")\n",
        "        print(f\"Model device: {next(self.model.parameters()).device}\")\n",
        "        print(f\"Model dtype: {next(self.model.parameters()).dtype}\")\n",
        "\n",
        "    def set_seed(self):\n",
        "        \"\"\"Set random seeds for reproducibility\"\"\"\n",
        "        random.seed(self.seed)\n",
        "        np.random.seed(self.seed)\n",
        "        torch.manual_seed(self.seed)\n",
        "        torch.cuda.manual_seed_all(self.seed)\n",
        "\n",
        "    def load_admission_patterns(self, file_path: str) -> List[Dict]:\n",
        "        \"\"\"Load medical admission patterns from JSONL file\"\"\"\n",
        "        patterns = []\n",
        "\n",
        "        try:\n",
        "            if not os.path.exists(file_path):\n",
        "                print(f\"Warning: File {file_path} not found. Creating sample data.\")\n",
        "                return self.create_sample_patterns()\n",
        "\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                for line_num, line in enumerate(f, 1):\n",
        "                    try:\n",
        "                        if line.strip():\n",
        "                            raw_pattern = json.loads(line.strip())\n",
        "                            processed_pattern = self.process_raw_pattern(raw_pattern)\n",
        "                            if processed_pattern:\n",
        "                                patterns.append(processed_pattern)\n",
        "                    except json.JSONDecodeError as e:\n",
        "                        print(f\"Warning: Skipping malformed JSON on line {line_num}: {e}\")\n",
        "                        continue\n",
        "\n",
        "            print(f\"Successfully loaded and processed {len(patterns)} patterns from {file_path}\")\n",
        "            return patterns\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading patterns: {e}\")\n",
        "            return self.create_sample_patterns()\n",
        "\n",
        "    def process_raw_pattern(self, raw_pattern: Dict) -> Optional[Dict]:\n",
        "        \"\"\"Process and structure raw pattern data from JSONL format\"\"\"\n",
        "        try:\n",
        "            question = raw_pattern.get('question', '')\n",
        "            answer_1 = raw_pattern.get('answer_1', {})\n",
        "            answer_2 = raw_pattern.get('answer_2', {})\n",
        "\n",
        "            if not all([question, answer_1, answer_2]):\n",
        "                return None\n",
        "\n",
        "            graph_answer = None\n",
        "            table_answer = None\n",
        "\n",
        "            if answer_1.get('type') == 'graph':\n",
        "                graph_answer = answer_1\n",
        "                table_answer = answer_2\n",
        "            elif answer_1.get('type') == 'table':\n",
        "                table_answer = answer_1\n",
        "                graph_answer = answer_2\n",
        "            else:\n",
        "                if 'nodes' in answer_1.get('content', {}):\n",
        "                    graph_answer = answer_1\n",
        "                    table_answer = answer_2\n",
        "                else:\n",
        "                    table_answer = answer_1\n",
        "                    graph_answer = answer_2\n",
        "\n",
        "            graph_representation = self.format_graph_representation(graph_answer.get('content', {}))\n",
        "            table_representation = self.format_table_representation(table_answer.get('content', {}))\n",
        "\n",
        "            return {\n",
        "                'question': question,\n",
        "                'graph_representation': graph_representation,\n",
        "                'tabular_representation': table_representation,\n",
        "                'graph_content': graph_answer.get('content', {}),\n",
        "                'table_content': table_answer.get('content', {}),\n",
        "                'raw_data': raw_pattern\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing pattern: {e}\")\n",
        "            return None\n",
        "\n",
        "    def format_graph_representation(self, graph_content: Dict) -> str:\n",
        "        \"\"\"Convert graph data structure to readable text format\"\"\"\n",
        "        nodes = graph_content.get('nodes', [])\n",
        "        edges = graph_content.get('edges', [])\n",
        "\n",
        "        representation = \"Graph Structure:\\n\"\n",
        "        if nodes:\n",
        "            representation += f\"Nodes: {', '.join(str(node) for node in nodes)}\\n\"\n",
        "\n",
        "        representation += \"Relationships:\\n\"\n",
        "        for edge in edges:\n",
        "            if len(edge) >= 3:\n",
        "                source, relation, target = edge[0], edge[1], edge[2]\n",
        "                representation += f\"  • {source} {relation} {target}\\n\"\n",
        "            elif len(edge) == 2:\n",
        "                source, target = edge[0], edge[1]\n",
        "                representation += f\"  • {source} → {target}\\n\"\n",
        "\n",
        "        return representation\n",
        "\n",
        "    def format_table_representation(self, table_content: Dict) -> str:\n",
        "        \"\"\"Convert tabular data structure to readable text format\"\"\"\n",
        "        headers = table_content.get('header', [])\n",
        "        rows = table_content.get('rows', [])\n",
        "\n",
        "        representation = \"Tabular Structure:\\n\"\n",
        "        if headers:\n",
        "            representation += f\"Headers: {', '.join(headers)}\\n\"\n",
        "\n",
        "        representation += \"Data rows:\\n\"\n",
        "        for i, row in enumerate(rows, 1):\n",
        "            if headers and len(row) == len(headers):\n",
        "                row_dict = dict(zip(headers, row))\n",
        "                representation += f\"  Row {i}: {row_dict}\\n\"\n",
        "            else:\n",
        "                representation += f\"  Row {i}: {row}\\n\"\n",
        "\n",
        "        return representation\n",
        "\n",
        "    def create_sample_patterns(self) -> List[Dict]:\n",
        "        \"\"\"Generate sample medical patterns for testing when no data file is available\"\"\"\n",
        "        return [\n",
        "            {\n",
        "                'question': 'Outline the unit transitions for Patient 10000032 during the URGENT admission.',\n",
        "                'graph_representation': 'Graph Structure:\\nNodes: Patient_10000032, Admission_22595853.0, URGENT, Emergency Department, Transplant\\nRelationships:\\n  • Patient_10000032 has_admission Admission_22595853.0\\n  • Admission_22595853.0 admission_type URGENT\\n  • Admission_22595853.0 transferred_to Emergency Department\\n  • Admission_22595853.0 transferred_to Transplant',\n",
        "                'tabular_representation': 'Tabular Structure:\\nHeaders: hadm_id, intime, outtime, eventtype, careunit\\nData rows:\\n  Row 1: {\\'hadm_id\\': \\'22595853.0\\', \\'intime\\': \\'2180-05-06 19:17:00\\', \\'outtime\\': \\'2180-05-06 23:30:00\\', \\'eventtype\\': \\'ED\\', \\'careunit\\': \\'Emergency Department\\'}\\n  Row 2: {\\'hadm_id\\': \\'22595853.0\\', \\'intime\\': \\'2180-05-06 23:30:00\\', \\'outtime\\': \\'2180-05-07 17:21:27\\', \\'eventtype\\': \\'admit\\', \\'careunit\\': \\'Transplant\\'}',\n",
        "                'graph_content': {},\n",
        "                'table_content': {},\n",
        "                'raw_data': {}\n",
        "            }\n",
        "        ]\n",
        "\n",
        "    def generate_structure_response(self, question: str, representation: str, structure_type: str) -> str:\n",
        "        \"\"\"Generate LLM response for a given data structure representation\"\"\"\n",
        "        prompt = f\"\"\"<|im_start|>system\n",
        "You are a medical expert analyzing patient unit transitions and admissions. Based on the {structure_type} representation provided, answer the clinical question with detailed analysis focusing on:\n",
        "\n",
        "- Patient flow and unit transitions\n",
        "- Timing and sequence of movements\n",
        "- Types of care units and their purposes\n",
        "- Clinical implications of transfers\n",
        "- Duration in each unit<|im_end|>\n",
        "\n",
        "<|im_start|>user\n",
        "Clinical Question: {question}\n",
        "\n",
        "{structure_type.title()} Representation:\n",
        "{representation}\n",
        "\n",
        "Provide a comprehensive analysis of the unit transitions based on this {structure_type} representation.<|im_end|>\n",
        "\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "\n",
        "        try:\n",
        "            inputs = self.tokenizer(\n",
        "                prompt,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                max_length=1024,\n",
        "                padding=False\n",
        "            )\n",
        "\n",
        "            device = next(self.model.parameters()).device\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=250,\n",
        "                    temperature=0.7,\n",
        "                    do_sample=True,\n",
        "                    top_p=0.9,\n",
        "                    top_k=20,\n",
        "                    pad_token_id=self.tokenizer.pad_token_id,\n",
        "                    eos_token_id=self.tokenizer.eos_token_id,\n",
        "                    repetition_penalty=1.05,\n",
        "                    length_penalty=1.0,\n",
        "                    use_cache=True\n",
        "                )\n",
        "\n",
        "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "\n",
        "            input_length = inputs['input_ids'].shape[1]\n",
        "            new_tokens = outputs[0][input_length:]\n",
        "            response = self.tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
        "\n",
        "            return response.strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during generation: {e}\")\n",
        "            return f\"Generated {structure_type} response: Analysis of unit transitions showing patient flow and timing patterns.\"\n",
        "\n",
        "    def create_llm_judge_prompt(self, question: str, graph_response: str, tabular_response: str) -> Tuple[str, Dict]:\n",
        "        \"\"\"Create evaluation prompt for LLM judge with randomized response order\"\"\"\n",
        "        if random.random() > 0.5:\n",
        "            response_a, response_b = graph_response, tabular_response\n",
        "            mapping = {'A': 'graph', 'B': 'tabular'}\n",
        "        else:\n",
        "            response_a, response_b = tabular_response, graph_response\n",
        "            mapping = {'A': 'tabular', 'B': 'graph'}\n",
        "\n",
        "        prompt = f\"\"\"<|im_start|>system\n",
        "You are an expert medical evaluator specializing in healthcare data analysis. Your task is to determine which of two clinical responses better answers the given question about patient unit transitions. Evaluate based on:\n",
        "\n",
        "1. Accuracy and completeness of information\n",
        "2. Clarity in describing unit transitions and timing\n",
        "3. Medical relevance and clinical insight\n",
        "4. Proper interpretation of the data structure\n",
        "5. Usefulness for healthcare decision-making\n",
        "\n",
        "Respond in JSON format with your evaluation and detailed rationale.<|im_end|>\n",
        "\n",
        "<|im_start|>user\n",
        "Question: {question}\n",
        "\n",
        "Response A: {response_a}\n",
        "\n",
        "Response B: {response_b}\n",
        "\n",
        "Which response (A or B) better answers the question about unit transitions? Consider the accuracy, completeness, clarity, and clinical relevance of each response.\n",
        "\n",
        "Respond in JSON format:\n",
        "{{\"better_response\": \"A\" or \"B\", \"rationale\": \"detailed explanation comparing both responses and why one is superior\", \"evaluation_criteria\": {{\"accuracy\": \"A\" or \"B\", \"completeness\": \"A\" or \"B\", \"clarity\": \"A\" or \"B\", \"clinical_relevance\": \"A\" or \"B\"}}, \"confidence\": \"high/medium/low\"}}<|im_end|>\n",
        "\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "\n",
        "        return prompt, mapping\n",
        "\n",
        "    def evaluate_with_llm_judge(self, pattern: Dict) -> Dict:\n",
        "        \"\"\"Evaluate a single pattern using LLM judge methodology\"\"\"\n",
        "        graph_response = self.generate_structure_response(\n",
        "            pattern['question'],\n",
        "            pattern['graph_representation'],\n",
        "            'graph'\n",
        "        )\n",
        "\n",
        "        tabular_response = self.generate_structure_response(\n",
        "            pattern['question'],\n",
        "            pattern['tabular_representation'],\n",
        "            'tabular'\n",
        "        )\n",
        "\n",
        "        judge_prompt, order_mapping = self.create_llm_judge_prompt(\n",
        "            pattern['question'],\n",
        "            graph_response,\n",
        "            tabular_response\n",
        "        )\n",
        "\n",
        "        judge_response = self.generate_judge_evaluation(judge_prompt)\n",
        "        judge_result = self.parse_judge_response(judge_response)\n",
        "\n",
        "        if judge_result['better_response'] == 'A':\n",
        "            winner = order_mapping['A']\n",
        "        elif judge_result['better_response'] == 'B':\n",
        "            winner = order_mapping['B']\n",
        "        else:\n",
        "            winner = 'unknown'\n",
        "\n",
        "        return {\n",
        "            'pattern': pattern,\n",
        "            'graph_response': graph_response,\n",
        "            'tabular_response': tabular_response,\n",
        "            'judge_evaluation': judge_result,\n",
        "            'winner': winner,\n",
        "            'order_mapping': order_mapping,\n",
        "            'judge_raw_response': judge_response\n",
        "        }\n",
        "\n",
        "    def generate_judge_evaluation(self, prompt: str) -> str:\n",
        "        \"\"\"Generate judge evaluation response using the LLM\"\"\"\n",
        "        try:\n",
        "            inputs = self.tokenizer(\n",
        "                prompt,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                max_length=1536,\n",
        "                padding=False\n",
        "            )\n",
        "\n",
        "            device = next(self.model.parameters()).device\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=400,\n",
        "                    temperature=0.3,\n",
        "                    do_sample=True,\n",
        "                    top_p=0.9,\n",
        "                    pad_token_id=self.tokenizer.pad_token_id,\n",
        "                    eos_token_id=self.tokenizer.eos_token_id,\n",
        "                    repetition_penalty=1.05,\n",
        "                    use_cache=True\n",
        "                )\n",
        "\n",
        "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "\n",
        "            input_length = inputs['input_ids'].shape[1]\n",
        "            new_tokens = outputs[0][input_length:]\n",
        "            response = self.tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
        "\n",
        "            return response.strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during judge generation: {e}\")\n",
        "            return '{\"better_response\": \"A\", \"rationale\": \"Error in evaluation\", \"evaluation_criteria\": {\"accuracy\": \"A\", \"completeness\": \"A\", \"clarity\": \"A\", \"clinical_relevance\": \"A\"}, \"confidence\": \"low\"}'\n",
        "\n",
        "    def parse_judge_response(self, response: str) -> Dict:\n",
        "        \"\"\"Parse and extract structured evaluation from judge response\"\"\"\n",
        "        try:\n",
        "            start_idx = response.find('{')\n",
        "            end_idx = response.rfind('}') + 1\n",
        "\n",
        "            if start_idx >= 0 and end_idx > start_idx:\n",
        "                json_str = response[start_idx:end_idx]\n",
        "                parsed = json.loads(json_str)\n",
        "\n",
        "                return {\n",
        "                    'better_response': parsed.get('better_response', 'unknown'),\n",
        "                    'rationale': parsed.get('rationale', 'No rationale provided'),\n",
        "                    'evaluation_criteria': parsed.get('evaluation_criteria', {}),\n",
        "                    'confidence': parsed.get('confidence', 'unknown')\n",
        "                }\n",
        "\n",
        "            if 'response a' in response.lower() or '\"a\"' in response.lower():\n",
        "                better = 'A'\n",
        "            elif 'response b' in response.lower() or '\"b\"' in response.lower():\n",
        "                better = 'B'\n",
        "            else:\n",
        "                better = 'unknown'\n",
        "\n",
        "            return {\n",
        "                'better_response': better,\n",
        "                'rationale': response[:300],\n",
        "                'evaluation_criteria': {},\n",
        "                'confidence': 'unknown'\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing judge response: {e}\")\n",
        "            return {\n",
        "                'better_response': 'unknown',\n",
        "                'rationale': 'Parse error',\n",
        "                'evaluation_criteria': {},\n",
        "                'confidence': 'low'\n",
        "            }\n",
        "\n",
        "    def run_comprehensive_evaluation(self, patterns: List[Dict], max_patterns: int = 10) -> Dict:\n",
        "        \"\"\"Execute comprehensive evaluation across multiple patterns using LLM judge\"\"\"\n",
        "        print(f\"Running comprehensive evaluation on {min(len(patterns), max_patterns)} patterns...\")\n",
        "\n",
        "        results = []\n",
        "        test_patterns = patterns[:max_patterns]\n",
        "\n",
        "        for i, pattern in enumerate(test_patterns):\n",
        "            print(f\"Evaluating pattern {i+1}/{len(test_patterns)}\")\n",
        "\n",
        "            try:\n",
        "                result = self.evaluate_with_llm_judge(pattern)\n",
        "                results.append(result)\n",
        "\n",
        "                print(f\"  - Winner: {result['winner']}\")\n",
        "                print(f\"  - Confidence: {result['judge_evaluation'].get('confidence', 'unknown')}\")\n",
        "                print(f\"  - Rationale preview: {result['judge_evaluation']['rationale'][:80]}...\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error evaluating pattern {i+1}: {e}\")\n",
        "                continue\n",
        "\n",
        "        analysis = self.analyze_comprehensive_results(results)\n",
        "\n",
        "        return {\n",
        "            'individual_results': results,\n",
        "            'analysis': analysis,\n",
        "            'metadata': {\n",
        "                'total_patterns': len(test_patterns),\n",
        "                'successful_evaluations': len(results),\n",
        "                'model_name': self.model_name,\n",
        "                'timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def analyze_comprehensive_results(self, results: List[Dict]) -> Dict:\n",
        "        \"\"\"Analyze and generate statistics from comprehensive evaluation results\"\"\"\n",
        "        if not results:\n",
        "            return {'error': 'No results to analyze'}\n",
        "\n",
        "        winners = [r['winner'] for r in results if r['winner'] != 'unknown']\n",
        "        winner_counts = Counter(winners)\n",
        "\n",
        "        confidence_levels = [r['judge_evaluation'].get('confidence', 'unknown') for r in results]\n",
        "        confidence_counts = Counter(confidence_levels)\n",
        "\n",
        "        criteria_analysis = {\n",
        "            'accuracy': Counter(),\n",
        "            'completeness': Counter(),\n",
        "            'clarity': Counter(),\n",
        "            'clinical_relevance': Counter()\n",
        "        }\n",
        "\n",
        "        for result in results:\n",
        "            criteria = result['judge_evaluation'].get('evaluation_criteria', {})\n",
        "            for criterion, winner in criteria.items():\n",
        "                if criterion in criteria_analysis:\n",
        "                    criteria_analysis[criterion][winner] += 1\n",
        "\n",
        "        rationale_lengths = [\n",
        "            len(r['judge_evaluation']['rationale'].split())\n",
        "            for r in results\n",
        "        ]\n",
        "\n",
        "        return {\n",
        "            'winner_distribution': dict(winner_counts),\n",
        "            'graph_wins': winner_counts.get('graph', 0),\n",
        "            'tabular_wins': winner_counts.get('tabular', 0),\n",
        "            'unknown_results': len([r for r in results if r['winner'] == 'unknown']),\n",
        "            'graph_win_rate': winner_counts.get('graph', 0) / len(winners) if winners else 0,\n",
        "            'tabular_win_rate': winner_counts.get('tabular', 0) / len(winners) if winners else 0,\n",
        "            'confidence_distribution': dict(confidence_counts),\n",
        "            'criteria_analysis': {k: dict(v) for k, v in criteria_analysis.items()},\n",
        "            'rationale_quality': {\n",
        "                'avg_rationale_length': np.mean(rationale_lengths) if rationale_lengths else 0,\n",
        "                'min_rationale_length': min(rationale_lengths) if rationale_lengths else 0,\n",
        "                'max_rationale_length': max(rationale_lengths) if rationale_lengths else 0\n",
        "            },\n",
        "            'total_evaluated': len(results)\n",
        "        }\n",
        "\n",
        "    def generate_evaluation_report(self, evaluation_results: Dict) -> str:\n",
        "        \"\"\"Generate comprehensive formatted evaluation report\"\"\"\n",
        "        analysis = evaluation_results['analysis']\n",
        "        metadata = evaluation_results['metadata']\n",
        "\n",
        "        report = \"=\"*60 + \"\\n\"\n",
        "        report += \"MEDICAL UNIT TRANSITIONS CON-J EVALUATION REPORT\\n\"\n",
        "        report += \"=\"*60 + \"\\n\\n\"\n",
        "\n",
        "        report += \"EXECUTIVE SUMMARY\\n\"\n",
        "        report += \"-\" * 20 + \"\\n\"\n",
        "        report += f\"• Evaluation timestamp: {metadata['timestamp']}\\n\"\n",
        "        report += f\"• Model used: {metadata['model_name']}\\n\"\n",
        "        report += f\"• Total patterns evaluated: {analysis['total_evaluated']}\\n\"\n",
        "        report += f\"• Successful evaluations: {metadata['successful_evaluations']}\\n\"\n",
        "\n",
        "        if 'error' not in analysis:\n",
        "            report += f\"• Graph structure wins: {analysis['graph_wins']} ({analysis['graph_win_rate']:.1%})\\n\"\n",
        "            report += f\"• Tabular structure wins: {analysis['tabular_wins']} ({analysis['tabular_win_rate']:.1%})\\n\"\n",
        "            report += f\"• Unknown/tied results: {analysis['unknown_results']}\\n\\n\"\n",
        "\n",
        "            report += \"PERFORMANCE ANALYSIS\\n\"\n",
        "            report += \"-\" * 20 + \"\\n\"\n",
        "            report += f\"Winner Distribution: {analysis['winner_distribution']}\\n\\n\"\n",
        "\n",
        "            report += \"CONFIDENCE ANALYSIS\\n\"\n",
        "            report += \"-\" * 15 + \"\\n\"\n",
        "            report += f\"Confidence Distribution: {analysis['confidence_distribution']}\\n\\n\"\n",
        "\n",
        "            report += \"EVALUATION CRITERIA BREAKDOWN\\n\"\n",
        "            report += \"-\" * 30 + \"\\n\"\n",
        "            for criterion, scores in analysis['criteria_analysis'].items():\n",
        "                if scores:\n",
        "                    report += f\"{criterion.title()}: {scores}\\n\"\n",
        "            report += \"\\n\"\n",
        "\n",
        "            report += \"JUDGE EVALUATION QUALITY\\n\"\n",
        "            report += \"-\" * 25 + \"\\n\"\n",
        "            rq = analysis['rationale_quality']\n",
        "            report += f\"• Average rationale length: {rq['avg_rationale_length']:.1f} words\\n\"\n",
        "            report += f\"• Rationale length range: {rq['min_rationale_length']}-{rq['max_rationale_length']} words\\n\\n\"\n",
        "\n",
        "        report += \"INDIVIDUAL CASE RESULTS\\n\"\n",
        "        report += \"-\" * 25 + \"\\n\"\n",
        "\n",
        "        for i, result in enumerate(evaluation_results['individual_results'][:5]):\n",
        "            report += f\"Case {i+1}:\\n\"\n",
        "            report += f\"  Question: {result['pattern']['question'][:80]}...\\n\"\n",
        "            report += f\"  Winner: {result['winner']}\\n\"\n",
        "            report += f\"  Confidence: {result['judge_evaluation'].get('confidence', 'unknown')}\\n\"\n",
        "            report += f\"  Rationale: {result['judge_evaluation']['rationale'][:120]}...\\n\\n\"\n",
        "\n",
        "        report += \"RECOMMENDATIONS\\n\"\n",
        "        report += \"-\" * 15 + \"\\n\"\n",
        "\n",
        "        if 'error' not in analysis:\n",
        "            if analysis['graph_win_rate'] > 0.7:\n",
        "                report += \"• Graph structures show strong advantage for unit transition queries\\n\"\n",
        "                report += \"• Consider why graph relationships excel for this task\\n\"\n",
        "            elif analysis['tabular_win_rate'] > 0.7:\n",
        "                report += \"• Tabular structures show strong advantage for unit transition queries\\n\"\n",
        "                report += \"• Consider why systematic tabular data excels for this task\\n\"\n",
        "            else:\n",
        "                report += \"• Results show balanced performance between structures\\n\"\n",
        "                report += \"• Both approaches have merit for different aspects of unit transitions\\n\"\n",
        "\n",
        "            high_confidence = analysis['confidence_distribution'].get('high', 0)\n",
        "            total_evals = sum(analysis['confidence_distribution'].values())\n",
        "            if high_confidence / total_evals < 0.5:\n",
        "                report += \"• Consider improving data quality or judge prompts for higher confidence\\n\"\n",
        "\n",
        "        report += \"\\nEND OF REPORT\\n\"\n",
        "        report += \"=\"*60\n",
        "\n",
        "        return report\n",
        "\n",
        "    def print_detailed_case_results(self, evaluation_results: Dict, case_numbers: List[int] = None):\n",
        "        \"\"\"Print detailed analysis results for specific evaluation cases\"\"\"\n",
        "        results = evaluation_results['individual_results']\n",
        "\n",
        "        if case_numbers is None:\n",
        "            case_numbers = list(range(min(3, len(results))))\n",
        "\n",
        "        for i in case_numbers:\n",
        "            if i >= len(results):\n",
        "                continue\n",
        "\n",
        "            result = results[i]\n",
        "            print(f\"\\n{'='*80}\")\n",
        "            print(f\"DETAILED CASE {i+1} RESULTS\")\n",
        "            print(f\"{'='*80}\")\n",
        "\n",
        "            print(f\"\\nQUESTION:\")\n",
        "            print(f\"{result['pattern']['question']}\")\n",
        "\n",
        "            print(f\"\\nGRAPH STRUCTURE RESPONSE:\")\n",
        "            print(f\"Representation:\\n{result['pattern']['graph_representation']}\")\n",
        "            print(f\"Generated Response: {result['graph_response']}\")\n",
        "\n",
        "            print(f\"\\nTABULAR STRUCTURE RESPONSE:\")\n",
        "            print(f\"Representation:\\n{result['pattern']['tabular_representation']}\")\n",
        "            print(f\"Generated Response: {result['tabular_response']}\")\n",
        "\n",
        "            print(f\"\\nJUDGE EVALUATION:\")\n",
        "            print(f\"Order mapping: {result['order_mapping']}\")\n",
        "            print(f\"Judge chose: Response {result['judge_evaluation']['better_response']}\")\n",
        "            print(f\"Final winner: {result['winner']}\")\n",
        "            print(f\"Confidence: {result['judge_evaluation'].get('confidence', 'unknown')}\")\n",
        "\n",
        "            criteria = result['judge_evaluation'].get('evaluation_criteria', {})\n",
        "            if criteria:\n",
        "                print(f\"Evaluation criteria breakdown: {criteria}\")\n",
        "\n",
        "            print(f\"\\nJUDGE RATIONALE:\")\n",
        "            print(f\"{result['judge_evaluation']['rationale']}\")\n",
        "\n",
        "            print(f\"\\nRAW JUDGE RESPONSE:\")\n",
        "            print(f\"{result['judge_raw_response']}\")\n",
        "            print(f\"{'='*80}\\n\")\n",
        "\n",
        "\n",
        "def run_llm_judge_evaluation_pipeline():\n",
        "    \"\"\"Execute the complete LLM judge evaluation pipeline for medical unit transitions\"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"MEDICAL UNIT TRANSITIONS CON-J EVALUATION PIPELINE\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    try:\n",
        "        evaluator = EnhancedMedicalConJEvaluator()\n",
        "        print(\"✓ Evaluator loaded successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Failed to load evaluator: {e}\")\n",
        "        return None\n",
        "\n",
        "    print(f\"\\n{'='*40}\")\n",
        "    print(\"LOADING UNIT TRANSITIONS DATASET\")\n",
        "    print(f\"{'='*40}\")\n",
        "\n",
        "    patterns = evaluator.load_admission_patterns('/content/sample_data/diagnosis_restructured.jsonl')\n",
        "\n",
        "    if not patterns:\n",
        "        print(\"No patterns loaded. Using sample data...\")\n",
        "        patterns = evaluator.create_sample_patterns()\n",
        "\n",
        "    print(f\"Loaded {len(patterns)} patterns for evaluation\")\n",
        "\n",
        "    if patterns:\n",
        "        print(f\"\\nSample pattern structure:\")\n",
        "        sample = patterns[0]\n",
        "        print(f\"Question: {sample['question']}\")\n",
        "        print(f\"Graph representation preview: {sample['graph_representation'][:200]}...\")\n",
        "        print(f\"Table representation preview: {sample['tabular_representation'][:200]}...\")\n",
        "\n",
        "    print(f\"\\n{'='*40}\")\n",
        "    print(\"RUNNING LLM JUDGE EVALUATION\")\n",
        "    print(f\"{'='*40}\")\n",
        "\n",
        "    max_patterns = min(50, len(patterns))\n",
        "    evaluation_results = evaluator.run_comprehensive_evaluation(patterns, max_patterns)\n",
        "\n",
        "    print(f\"\\n{'='*40}\")\n",
        "    print(\"EVALUATION REPORT\")\n",
        "    print(f\"{'='*40}\")\n",
        "\n",
        "    report = evaluator.generate_evaluation_report(evaluation_results)\n",
        "    print(report)\n",
        "\n",
        "    print(f\"\\n{'='*40}\")\n",
        "    print(\"DETAILED CASE ANALYSIS\")\n",
        "    print(f\"{'='*40}\")\n",
        "\n",
        "    evaluator.print_detailed_case_results(evaluation_results, [0, 1])\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    results_file = f\"unit_transitions_evaluation_{timestamp}.json\"\n",
        "\n",
        "    try:\n",
        "        serializable_results = {\n",
        "            'analysis': evaluation_results['analysis'],\n",
        "            'metadata': evaluation_results['metadata'],\n",
        "            'individual_results_summary': [\n",
        "                {\n",
        "                    'question': r['pattern']['question'],\n",
        "                    'winner': r['winner'],\n",
        "                    'confidence': r['judge_evaluation'].get('confidence', 'unknown'),\n",
        "                    'rationale_preview': r['judge_evaluation']['rationale'][:200],\n",
        "                    'evaluation_criteria': r['judge_evaluation'].get('evaluation_criteria', {})\n",
        "                }\n",
        "                for r in evaluation_results['individual_results']\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        with open(results_file, 'w') as f:\n",
        "            json.dump(serializable_results, f, indent=2)\n",
        "\n",
        "        print(f\"\\n✓ Results saved to {results_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not save results to file: {e}\")\n",
        "\n",
        "    print(f\"\\n✓ Unit Transitions LLM Judge evaluation pipeline completed!\")\n",
        "    return evaluation_results\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = run_llm_judge_evaluation_pipeline()"
      ]
    }
  ]
}